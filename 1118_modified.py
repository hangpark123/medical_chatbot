import gradio as gr
import os
from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores.chroma import Chroma
from langchain.prompts.prompt import PromptTemplate
from langchain_community.llms import Ollama

# ë°ì´í„° ë¡œë”© ë° ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
persist_directory = "chroma_db"
loader = CSVLoader(file_path="src/medicine_update.csv", encoding='UTF-8')
docs = loader.load()
print("[DEBUG] Loaded Documents:", docs)


# ì„ë² ë”© ì„¤ì •
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cuda'},
    encode_kwargs={'normalize_embeddings': True}
)

# í…ìŠ¤íŠ¸ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)   
splits = text_splitter.split_documents(docs)
print("[DEBUG] Split Documents:", splits)


def process_in_batches(documents, embeddings, persist_directory, batch_size=5000):
    if not os.path.exists(persist_directory):
        vector_store = None
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            if vector_store is None:
                vector_store = Chroma.from_documents(batch, embeddings, persist_directory=persist_directory)
                vector_store.persist()
            else: 
                vector_store.add_documents(batch)
        return vector_store
    else:
        return Chroma(persist_directory=persist_directory, embedding_function=embeddings)

vector_store = process_in_batches(splits, embeddings, persist_directory)
print("[DEBUG] Vector Store:", vector_store)
retriever = vector_store.as_retriever()

# Ollama ëª¨ë¸ ì´ˆê¸°í™”
chat_llm = Ollama(
    base_url='http://127.0.0.1:11434',
    model='kollama4'
)

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
request_prompt = PromptTemplate(
    template="""ì‚¬ìš©ìì˜ ì¦ìƒì„ ë¶„ì„í•˜ê³  ì ì ˆí•œ ì•½í’ˆì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. ì•½ì˜ ì œí’ˆëª…ì„ ê·¸ëŒ€ë¡œ í‘œì‹œí•˜ì„¸ìš”.
    ì¦ìƒ: {question}
    
    ì•„ë˜ì˜ ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ê°€ì¥ ì í•©í•œ ì•½í’ˆ 3ê°€ì§€ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”:
    ë§¥ë½: {context}
    
    ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
    
    [ë¶„ì„ëœ ì¦ìƒ]
    ì‚¬ìš©ìì˜ ì¦ìƒì— ëŒ€í•œ ê°„ë‹¨í•œ ë¶„ì„
    
    [ì¶”ì²œ ì•½í’ˆ]
    1. [ì œí’ˆëª…] - [ì£¼ìš” íš¨ëŠ¥]
    2. [ì œí’ˆëª…] - [ì£¼ìš” íš¨ëŠ¥]
    3. [ì œí’ˆëª…] - [ì£¼ìš” íš¨ëŠ¥]
    
    [ì£¼ì˜ì‚¬í•­]
    ì•½í’ˆ ë³µìš© ì‹œ ì£¼ì˜í•´ì•¼ í•  ì ë“¤
    """,
    input_variables=['context', 'question']
)

def format_ret_docs(ret_docs):
    print("[DEBUG] Retrieved Documents:", ret_docs) #testingìš© ì½”ë“œ
    return "\n\n".join(doc.page_content for doc in ret_docs)

# RAG ì²´ì¸ ì„¤ì •
rag_chain = (
    {'context': retriever | format_ret_docs, 'question': RunnablePassthrough()}
    | request_prompt
    | chat_llm
    | StrOutputParser()
)

def initialize_state():
    return {
        "chat_history": [(None, "ì•ˆë…•í•˜ì„¸ìš”. ì–´ë–¤ ì¦ìƒì´ ìˆìœ¼ì‹ ì§€ ìì„¸íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.")]
    }

def get_recommendation(sentence):
    
    question = f"ì‚¬ìš©ìê°€ ì„ íƒí•œ ì¦ìƒì€ '{sentence}'ì…ë‹ˆë‹¤. ì˜ì‹¬ë˜ëŠ” ì¦ìƒì„ í•œ ë‹¨ì–´ë¡œ í‘œí˜„í•˜ì„¸ìš”."\
    "ê·¸ ë‹¤ìŒ ì¦ìƒê³¼ ê´€ë ¨ëœ ì•½ì„ RAG ë°ì´í„°ë² ì´ìŠ¤ì— ì˜¬ë ¤ì ¸ìˆëŠ” ë°ì´í„°ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”." \
    "ë§Œì•½ ê´€ë ¨ëœ ì•½ì´ ì—†ë‹¤ë©´, ë¹„ìŠ·í•œ íš¨ëŠ¥ì´ ìˆëŠ” ì•½ì„ ì œì•ˆí•´ì£¼ì„¸ìš”."
    response = rag_chain.invoke(question)
    return response

def conversation(message, state):
    if state is None:
        state = initialize_state()
    
    chat_history = state["chat_history"]
    
    if message.strip().lower() == "ì²˜ìŒ":
        state = initialize_state()
        return "ì•ˆë…•í•˜ì„¸ìš”. ì–´ë–¤ ì¦ìƒì´ ìˆìœ¼ì‹ ì§€ ìì„¸íˆ ë§ì”€í•´ ì£¼ì„¸ìš”.", state, state["chat_history"]
    
    ###----- added by Joo
    sentence = message.strip()
    print("[DEBUG]ì…ë ¥ëœ sentence:", sentence) #testingìš© ì½”ë“œ
    response = f"ì…ë ¥ëœ ì¦ìƒ: {sentence}. ì´ì— ëŒ€í•œ ì•½ì„ ì¶”ì²œí•˜ê² ìŠµë‹ˆë‹¤."

    recommendation = get_recommendation(message)
    response += "\n" + recommendation + "\në‹¤ë¥¸ ì¦ìƒì„ ë¬¸ì˜í•˜ì‹œë ¤ë©´ 'ì²˜ìŒ'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."
    
    chat_history.append((message, response))
    return response, state, chat_history

# Gradio ì¸í„°í˜ì´ìŠ¤
with gr.Blocks(theme=gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="gray",
), css="""
    #chatbot { 
        background-color: #f8fafc;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
    }
    .message { font-size: 1.1rem !important; }
    .input-box { 
        border-radius: 8px !important;
        border: 1px solid #e2e8f0 !important;
        padding: 12px !important;
        font-size: 1rem !important;
    }
""") as demo:
    gr.Markdown(
        """
        <div style="text-align: center;">
            <h1 style="font-size: 2.8rem; font-weight: 700; color: #2d5ca9; margin-bottom: 1rem;">
                ğŸ¥ AI í—¬ìŠ¤ì¼€ì–´ ì–´ì‹œìŠ¤í„´íŠ¸
            </h1>
            <p style="font-size: 1.3rem; color: #4a5568; line-height: 1.6;">
                ì¦ìƒì„ ìì„¸íˆ ë§ì”€í•´ ì£¼ì‹œë©´ ì ì ˆí•œ ì•½í’ˆì„ ì¶”ì²œí•´ ë“œë¦½ë‹ˆë‹¤.
            </p>
        </div>
        """
    )
    
    chatbot = gr.Chatbot(
        [],
        elem_id="chatbot",
        bubble_full_width=False,
        avatar_images=("user.png", "bot.png"),
        height=600,
    )

    with gr.Row():
        with gr.Column(scale=9):
            msg = gr.Textbox(
                placeholder="ì¦ìƒì„ ìì„¸íˆ ì…ë ¥í•´ ì£¼ì„¸ìš”...",
                show_label=False,
                container=False,
                elem_classes="input-box",
                min_width=600,
            )
        with gr.Column(scale=1, min_width=100):
            send_btn = gr.Button(
                "ì „ì†¡",
                variant="primary"
            )

    with gr.Row():
        clear_btn = gr.Button(
            "ìƒˆë¡œìš´ ìƒë‹´ ì‹œì‘",
            variant="secondary",
            size="sm"
        )

    state = gr.State(initialize_state())

    def clear_conversation():
        new_state = initialize_state()
        return new_state["chat_history"], new_state

    def handle_message(message, state):
        response, new_state, chat_history = conversation(message, state)
        return "", new_state, chat_history

    clear_btn.click(clear_conversation, [], [chatbot, state])
    send_btn.click(handle_message, [msg, state], [msg, state, chatbot])
    msg.submit(handle_message, [msg, state], [msg, state, chatbot])
    demo.load(lambda: initialize_state()["chat_history"], outputs=[chatbot])

demo.launch()